{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b1856d-f8df-4f21-b6c0-1784647adf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (2.5.0.post0)\n",
      "Requirement already satisfied: torch in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: optuna in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (4.2.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: pandas in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: seaborn in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: pytorch-forecasting in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (1.3.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from pytorch-lightning) (4.66.4)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from pytorch-lightning) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.3.1)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from pytorch-lightning) (1.6.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from pytorch-lightning) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from pytorch-lightning) (4.12.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from pytorch-lightning) (0.13.1)\n",
      "Requirement already satisfied: filelock in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: networkx in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from optuna) (1.15.1)\n",
      "Requirement already satisfied: colorlog in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from optuna) (2.0.30)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: lightning<3.0.0,>=2.0.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from pytorch-forecasting) (2.5.0.post0)\n",
      "Requirement already satisfied: Mako in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.3)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-lightning torch optuna scikit-learn pandas matplotlib seaborn pytorch-forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cb79beb-4e46-49e5-91f6-71ab1f1036cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.17.0\n",
      "  Using cached tensorflow-2.17.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: pandas in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: matplotlib in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: seaborn in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: keras-tuner in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (1.4.7)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from tensorflow==2.17.0) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.17.0)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow==2.17.0)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.17.0)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.17.0)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from tensorflow==2.17.0) (3.11.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.17.0)\n",
      "  Using cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow==2.17.0)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.17.0)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from tensorflow==2.17.0) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from tensorflow==2.17.0) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from tensorflow==2.17.0) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from tensorflow==2.17.0) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from tensorflow==2.17.0) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.17.0)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from tensorflow==2.17.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from tensorflow==2.17.0) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from tensorflow==2.17.0) (1.71.0)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow==2.17.0)\n",
      "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.2.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from tensorflow==2.17.0) (3.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kt-legacy in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.17.0) (0.43.0)\n",
      "Requirement already satisfied: rich in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow==2.17.0) (13.3.5)\n",
      "Requirement already satisfied: namex in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.0) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/Intel/Desktop/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow==2.17.0) (0.1.0)\n",
      "Downloading tensorflow-2.17.0-cp312-cp312-macosx_12_0_arm64.whl (236.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.3/236.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "Downloading ml_dtypes-0.4.1-cp312-cp312-macosx_10_9_universal2.whl (405 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, opt-einsum, ml-dtypes, google-pasta, gast, astunparse, tensorboard, tensorflow\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml_dtypes 0.5.1\n",
      "    Uninstalling ml_dtypes-0.5.1:\n",
      "      Successfully uninstalled ml_dtypes-0.5.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.19.0\n",
      "    Uninstalling tensorboard-2.19.0:\n",
      "      Successfully uninstalled tensorboard-2.19.0\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 libclang-18.1.1 ml-dtypes-0.4.1 opt-einsum-3.4.0 tensorboard-2.17.1 tensorflow-2.17.0 termcolor-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.17.0 pandas numpy scikit-learn matplotlib seaborn keras-tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f844e07d-0efe-432c-9836-00565c05d0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 389895 entries, 0 to 389894\n",
      "Data columns (total 20 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   OBJECTID                389895 non-null  int64  \n",
      " 1   INCIDENT_ID             389895 non-null  int64  \n",
      " 2   OFFENSE_ID              389895 non-null  int64  \n",
      " 3   OFFENSE_CODE            389895 non-null  int64  \n",
      " 4   OFFENSE_CODE_EXTENSION  389895 non-null  int64  \n",
      " 5   OFFENSE_TYPE_ID         389895 non-null  object \n",
      " 6   OFFENSE_CATEGORY_ID     389895 non-null  object \n",
      " 7   FIRST_OCCURRENCE_DATE   389895 non-null  object \n",
      " 8   REPORTED_DATE           389895 non-null  object \n",
      " 9   INCIDENT_ADDRESS        389895 non-null  object \n",
      " 10  GEO_X                   389895 non-null  int64  \n",
      " 11  GEO_Y                   389895 non-null  int64  \n",
      " 12  GEO_LON                 389895 non-null  float64\n",
      " 13  GEO_LAT                 389895 non-null  float64\n",
      " 14  DISTRICT_ID             389895 non-null  object \n",
      " 15  PRECINCT_ID             389895 non-null  int64  \n",
      " 16  NEIGHBORHOOD_ID         389895 non-null  object \n",
      " 17  VICTIM_COUNT            389895 non-null  int64  \n",
      " 18  x                       389895 non-null  float64\n",
      " 19  y                       389895 non-null  float64\n",
      "dtypes: float64(4), int64(9), object(7)\n",
      "memory usage: 59.5+ MB\n",
      "None\n",
      "   OBJECTID  INCIDENT_ID         OFFENSE_ID  OFFENSE_CODE  \\\n",
      "0         1   2020454617   2020454617299900          2999   \n",
      "1         2  20206013877  20206013877299900          2999   \n",
      "2         3    202210816    202210816299900          2999   \n",
      "3         4   2021159354   2021159354299900          2999   \n",
      "4         5   2020470873   2020470873299900          2999   \n",
      "\n",
      "   OFFENSE_CODE_EXTENSION          OFFENSE_TYPE_ID OFFENSE_CATEGORY_ID  \\\n",
      "0                       0  criminal-mischief-other     public-disorder   \n",
      "1                       0  criminal-mischief-other     public-disorder   \n",
      "2                       0  criminal-mischief-other     public-disorder   \n",
      "3                       0  criminal-mischief-other     public-disorder   \n",
      "4                       0  criminal-mischief-other     public-disorder   \n",
      "\n",
      "  FIRST_OCCURRENCE_DATE        REPORTED_DATE           INCIDENT_ADDRESS  \\\n",
      "0   2020-07-26 16:00:00  2020-07-27 22:09:00        15987 E RANDOLPH PL   \n",
      "1   2020-10-10 04:55:00  2020-10-14 15:20:00          6980 E GIRARD AVE   \n",
      "2   2022-01-08 00:01:00  2022-01-08 00:01:00        3145 W ARKANSAS AVE   \n",
      "3   2021-03-20 00:38:00  2021-03-20 00:38:00  2300 BLOCK N FEDERAL BLVD   \n",
      "4   2020-08-04 11:45:00  2020-08-04 16:28:00            900 BLK 10TH ST   \n",
      "\n",
      "   GEO_X  GEO_Y     GEO_LON    GEO_LAT DISTRICT_ID  PRECINCT_ID  \\\n",
      "0      0      0 -104.801111  39.797827           5          522   \n",
      "1      0      0 -104.905807  39.654774           3          323   \n",
      "2      0      0 -105.027614  39.691784           4          421   \n",
      "3      0      0 -105.025019  39.751335           1          121   \n",
      "4      0      0 -105.001965  39.740860           1          123   \n",
      "\n",
      "              NEIGHBORHOOD_ID  VICTIM_COUNT             x             y  \n",
      "0  gateway-green-valley-ranch             1  3.196379e+06  1.716245e+06  \n",
      "1                     hampden             1  3.167302e+06  1.663927e+06  \n",
      "2                     mar-lee             1  3.132936e+06  1.677207e+06  \n",
      "3              jefferson-park             1  3.133553e+06  1.698903e+06  \n",
      "4                     auraria             1  3.140056e+06  1.695122e+06  \n",
      "OBJECTID                  0\n",
      "INCIDENT_ID               0\n",
      "OFFENSE_ID                0\n",
      "OFFENSE_CODE              0\n",
      "OFFENSE_CODE_EXTENSION    0\n",
      "OFFENSE_TYPE_ID           0\n",
      "OFFENSE_CATEGORY_ID       0\n",
      "FIRST_OCCURRENCE_DATE     0\n",
      "REPORTED_DATE             0\n",
      "INCIDENT_ADDRESS          0\n",
      "GEO_X                     0\n",
      "GEO_Y                     0\n",
      "GEO_LON                   0\n",
      "GEO_LAT                   0\n",
      "DISTRICT_ID               0\n",
      "PRECINCT_ID               0\n",
      "NEIGHBORHOOD_ID           0\n",
      "VICTIM_COUNT              0\n",
      "x                         0\n",
      "y                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"/Users/Intel/Desktop/spring 2025/CAPSTONE/cleaned_data.csv\")\n",
    "\n",
    "# Display basic information\n",
    "print(df.info())\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45a3641f-41cf-4689-8619-2761e478fdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FIRST_OCCURRENCE_DATE  time_idx\n",
      "0   2019-01-02 00:01:00         0\n",
      "1   2019-01-02 00:10:00         0\n",
      "2   2019-01-02 01:11:00         0\n",
      "3   2019-01-02 01:30:00         0\n",
      "4   2019-01-02 01:30:00         0\n"
     ]
    }
   ],
   "source": [
    "# Ensure FIRST_OCCURRENCE_DATE is in datetime format\n",
    "df[\"FIRST_OCCURRENCE_DATE\"] = pd.to_datetime(df[\"FIRST_OCCURRENCE_DATE\"])\n",
    "\n",
    "# Sort by date (important for sequential modeling)\n",
    "df = df.sort_values(\"FIRST_OCCURRENCE_DATE\").reset_index(drop=True)\n",
    "\n",
    "# Create a time index (days since the first record)\n",
    "df[\"time_idx\"] = (df[\"FIRST_OCCURRENCE_DATE\"] - df[\"FIRST_OCCURRENCE_DATE\"].min()).dt.days\n",
    "\n",
    "# Verify that time_idx exists\n",
    "print(df[[\"FIRST_OCCURRENCE_DATE\", \"time_idx\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c153732-7cbe-4d13-ac97-7badb987e955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TimeSeriesDataSet created with Sliding Window Approach!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "\n",
    "# Define forecasting parameters\n",
    "max_encoder_length = 90  # Past 90 days used for training\n",
    "max_prediction_length = 30  # Predict the next 30 days\n",
    "\n",
    "# Define the TimeSeriesDataSet\n",
    "dataset = TimeSeriesDataSet(\n",
    "    df,\n",
    "    time_idx=\"time_idx\",  # Time index for sequential data\n",
    "    target=\"VICTIM_COUNT\",  # Predicting crime count\n",
    "    group_ids=[\"DISTRICT_ID\"],  # Grouping by districts\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_known_reals=[\"time_idx\"],  # Time index as known future values\n",
    "    time_varying_unknown_reals=[\"VICTIM_COUNT\", \"GEO_LON\", \"GEO_LAT\"],  # Unknown values we predict\n",
    "    static_categoricals=[\"DISTRICT_ID\", \"NEIGHBORHOOD_ID\", \"OFFENSE_CATEGORY_ID\"],  # Static categorical data\n",
    "    allow_missing_timesteps=True,  # Allow for missing days\n",
    ")\n",
    "\n",
    "print(\"✅ TimeSeriesDataSet created with Sliding Window Approach!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8e45c80-d655-47da-a2fb-07b6dcf3aa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data successfully split for training, validation, and testing!\n"
     ]
    }
   ],
   "source": [
    "#Train Set: Use older dates for training.\n",
    "#Validation Set: Use a recent portion for tuning.\n",
    "#Test Set: Use the most recent unseen data.\n",
    "# Split dataset into training (70%), validation (20%), and test (10%) based on time\n",
    "train_idx = int(len(df) * 0.7)\n",
    "val_idx = int(len(df) * 0.9)\n",
    "\n",
    "# Training, validation, and testing sets\n",
    "train_data = TimeSeriesDataSet.from_dataset(dataset, df.iloc[:train_idx])\n",
    "val_data = TimeSeriesDataSet.from_dataset(dataset, df.iloc[train_idx:val_idx])\n",
    "test_data = TimeSeriesDataSet.from_dataset(dataset, df.iloc[val_idx:])\n",
    "\n",
    "# Convert to PyTorch DataLoaders\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = train_data.to_dataloader(train=True, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = val_data.to_dataloader(train=False, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = test_data.to_dataloader(train=False, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"✅ Data successfully split for training, validation, and testing!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad21b81a-c8ce-4527-85b2-278d01b5e1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow TFT model built and compiled!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# ------------------- Feature Engineering -------------------\n",
    "\n",
    "# Convert FIRST_OCCURRENCE_DATE to datetime\n",
    "df[\"FIRST_OCCURRENCE_DATE\"] = pd.to_datetime(df[\"FIRST_OCCURRENCE_DATE\"])\n",
    "\n",
    "# Sort by date and create time_idx\n",
    "df = df.sort_values(\"FIRST_OCCURRENCE_DATE\").reset_index(drop=True)\n",
    "df[\"time_idx\"] = (df[\"FIRST_OCCURRENCE_DATE\"] - df[\"FIRST_OCCURRENCE_DATE\"].min()).dt.days\n",
    "\n",
    "# Extract useful time-based features\n",
    "df[\"year\"] = df[\"FIRST_OCCURRENCE_DATE\"].dt.year\n",
    "df[\"month\"] = df[\"FIRST_OCCURRENCE_DATE\"].dt.month\n",
    "df[\"day\"] = df[\"FIRST_OCCURRENCE_DATE\"].dt.day\n",
    "df[\"weekday\"] = df[\"FIRST_OCCURRENCE_DATE\"].dt.weekday\n",
    "\n",
    "# ------------------- Encoding and Scaling -------------------\n",
    "\n",
    "# Encode categorical columns\n",
    "for col in [\"DISTRICT_ID\", \"NEIGHBORHOOD_ID\", \"OFFENSE_CATEGORY_ID\"]:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "# Scale continuous columns\n",
    "scaler = StandardScaler()\n",
    "df[[\"GEO_LON\", \"GEO_LAT\", \"VICTIM_COUNT\"]] = scaler.fit_transform(df[[\"GEO_LON\", \"GEO_LAT\", \"VICTIM_COUNT\"]])\n",
    "\n",
    "# ------------------- Data Split -------------------\n",
    "\n",
    "train_df = df[df[\"FIRST_OCCURRENCE_DATE\"] < \"2022-01-01\"]\n",
    "val_df = df[(df[\"FIRST_OCCURRENCE_DATE\"] >= \"2022-01-01\") & (df[\"FIRST_OCCURRENCE_DATE\"] < \"2022-07-01\")]\n",
    "test_df = df[df[\"FIRST_OCCURRENCE_DATE\"] >= \"2022-07-01\"]\n",
    "\n",
    "# ------------------- TFT Model (Keras Custom Implementation) -------------------\n",
    "\n",
    "def build_tft_model():\n",
    "    input_seq = keras.Input(shape=(90, 7))\n",
    "\n",
    "    # Static embedding input (optional in this simplified version)\n",
    "    lstm_out = layers.LSTM(32, return_sequences=True)(input_seq)\n",
    "    attention = layers.Attention()([lstm_out, lstm_out])\n",
    "    flatten = layers.GlobalAveragePooling1D()(attention)\n",
    "    \n",
    "    output = layers.Dense(1)(flatten)\n",
    "\n",
    "    model = keras.Model(inputs=input_seq, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "model = build_tft_model()\n",
    "\n",
    "print(\"✅ TensorFlow TFT model built and compiled!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8518f33d-66f4-48bf-9dd5-0eedc5587704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sliding window dataset ready for TFT model!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "encoder_length = 90  # past days\n",
    "decoder_length = 30  # predict next days\n",
    "window_size = encoder_length + decoder_length\n",
    "\n",
    "# Create the feature array (we'll focus on \"VICTIM_COUNT\" for now)\n",
    "data = df[['VICTIM_COUNT']].values.astype(np.float32)\n",
    "\n",
    "# Create tf.data.Dataset with sliding window\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "dataset = dataset.window(window_size, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_size))\n",
    "\n",
    "# Split into encoder (input) and decoder (target)\n",
    "dataset = dataset.map(lambda window: (\n",
    "    window[:encoder_length],  # encoder input\n",
    "    window[encoder_length:]   # decoder target\n",
    "))\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"✅ Sliding window dataset ready for TFT model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f687b366-46dc-4f24-91ee-cd3c8e294e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your dataframe based on time\n",
    "train_df = df[df['FIRST_OCCURRENCE_DATE'] < \"2022-01-01\"]\n",
    "val_df = df[(df['FIRST_OCCURRENCE_DATE'] >= \"2022-01-01\") & (df['FIRST_OCCURRENCE_DATE'] < \"2022-07-01\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b5ebfdef-98a5-4723-822c-e58e91be282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and feature columns\n",
    "target = \"VICTIM_COUNT\"\n",
    "features = ['time_idx', 'GEO_LON', 'GEO_LAT', 'VICTIM_COUNT', 'DISTRICT_ID', 'NEIGHBORHOOD_ID', 'OFFENSE_CATEGORY_ID']\n",
    " # Add more features if needed like district/other static vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36fcd681-9c25-4e75-b2a1-068aef5c4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_df[features].values, train_df[target].values))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_df[features].values, val_df[target].values))\n",
    "\n",
    "# Apply batching and prefetching\n",
    "tf_dataset = train_dataset.shuffle(1024).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "tf_val_dataset = val_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6101a646-e147-4042-9f99-981ea74dd55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tf.data.Dataset objects for training and validation\n",
    "tf_dataset = dataset.shuffle(1024).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "tf_val_dataset = val_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed7d910b-926d-4dca-8e40-f85dc88feb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape before batching: Add feature dimension (assuming 1 feature for now)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_df[features].values[..., np.newaxis], train_df[target].values))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_df[features].values[..., np.newaxis], val_df[target].values))\n",
    "\n",
    "# Batch and prefetch\n",
    "tf_dataset = train_dataset.shuffle(1024).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "tf_val_dataset = val_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3e53cdf-15ee-4597-8ce9-6e6b6dfc9c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['time_idx', 'GEO_LON', 'GEO_LAT', 'VICTIM_COUNT', 'DISTRICT_ID', 'NEIGHBORHOOD_ID', 'OFFENSE_CATEGORY_ID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "754dcd9c-ffb2-49cd-984f-5bcdd907fa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186018, 90, 7)\n",
      "(186018,)\n"
     ]
    }
   ],
   "source": [
    "# Sliding window on features and target\n",
    "def create_sliding_windows(df, seq_length, target_col):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_length):\n",
    "        X.append(df[features].iloc[i:i+seq_length].values)\n",
    "        y.append(df[target_col].iloc[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Apply to train and validation\n",
    "X_train, y_train = create_sliding_windows(train_df, 90, target)\n",
    "X_val, y_val = create_sliding_windows(val_df, 90, target)\n",
    "\n",
    "print(X_train.shape)  # Should be (num_samples, 90, 7)\n",
    "print(y_train.shape)  # Should be (num_samples,)\n",
    "\n",
    "# TensorFlow dataset\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "tf_val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(64).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f5a70a7c-ae62-41b5-ba9b-418bc002f6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m2907/2907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 13ms/step - loss: 0.8834 - val_loss: 0.9307\n",
      "Epoch 2/30\n",
      "\u001b[1m2907/2907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 13ms/step - loss: 0.8812 - val_loss: 0.9307\n",
      "Epoch 3/30\n",
      "\u001b[1m2907/2907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 13ms/step - loss: 0.8796 - val_loss: 0.9307\n",
      "Epoch 4/30\n",
      "\u001b[1m2907/2907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 14ms/step - loss: 0.8789 - val_loss: 0.9307\n",
      "Epoch 5/30\n",
      "\u001b[1m2907/2907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 13ms/step - loss: 0.8789 - val_loss: 0.9307\n",
      "Epoch 6/30\n",
      "\u001b[1m2907/2907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 15ms/step - loss: 0.8787 - val_loss: 0.9307\n",
      "✅ TFT model training complete!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,  # Stop if no improvement for 5 epochs\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    tf_dataset,\n",
    "    validation_data=tf_val_dataset,\n",
    "    epochs=30,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"✅ TFT model training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cfdf655e-45f0-4f10-a3ee-8159e308876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64  # increase from 16\n",
    "attention_head_size = 8  # increase from 4\n",
    "dropout = 0.2  # slight increase for regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8a397cec-1b0b-41af-9a38-c89da46ba77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_of_week'] = df['FIRST_OCCURRENCE_DATE'].dt.dayofweek\n",
    "df['month'] = df['FIRST_OCCURRENCE_DATE'].dt.month\n",
    "df['week_of_year'] = df['FIRST_OCCURRENCE_DATE'].dt.isocalendar().week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3061ab27-8551-44dc-aaeb-9739f559781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce368f5f-21f2-4bab-acdc-133dcfba59ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.early_stopping.EarlyStopping at 0x303cafa40>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e3f5fe7-25de-49ba-b223-e4c8ecd5dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering BEFORE splitting\n",
    "df['day_of_week'] = df['FIRST_OCCURRENCE_DATE'].dt.dayofweek\n",
    "df['week_of_year'] = df['FIRST_OCCURRENCE_DATE'].dt.isocalendar().week\n",
    "\n",
    "# Scaling features\n",
    "features = [\"GEO_LON\", \"GEO_LAT\", \"time_idx\", \"day_of_week\", \"week_of_year\"]\n",
    "target = \"VICTIM_COUNT\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "# Now split into train and validation\n",
    "train_size = int(len(df) * 0.7)\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:]\n",
    "\n",
    "# Prepare TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_df[features].values, train_df[target].values))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_df[features].values, val_df[target].values))\n",
    "\n",
    "# Batch and prefetch\n",
    "train_dataset = train_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a9d9ca3-ff16-4aa1-9d24-5cdba8ce02bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVICTIM_COUNT\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Step 2: Prepare sliding window datasets correctly (no extra wrapper needed)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mtimeseries_dataset_from_array(\n\u001b[1;32m      7\u001b[0m     data\u001b[38;5;241m=\u001b[39mtrain_df[features]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m      8\u001b[0m     targets\u001b[38;5;241m=\u001b[39mtrain_df[target]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m      9\u001b[0m     sequence_length\u001b[38;5;241m=\u001b[39msequence_length,\n\u001b[1;32m     10\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mtimeseries_dataset_from_array(\n\u001b[1;32m     14\u001b[0m     data\u001b[38;5;241m=\u001b[39mval_df[features]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m     15\u001b[0m     targets\u001b[38;5;241m=\u001b[39mval_df[target]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m     16\u001b[0m     sequence_length\u001b[38;5;241m=\u001b[39msequence_length,\n\u001b[1;32m     17\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Prefetch to improve performance\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.12/site-packages/keras/src/utils/timeseries_dataset_utils.py:231\u001b[0m, in \u001b[0;36mtimeseries_dataset_from_array\u001b[0;34m(data, targets, sequence_length, sequence_stride, sampling_rate, batch_size, shuffle, seed, start_index, end_index)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# For each initial window position, generates indices of the window elements\u001b[39;00m\n\u001b[1;32m    220\u001b[0m indices \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip(\n\u001b[1;32m    221\u001b[0m     (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;28mlen\u001b[39m(start_positions)), positions_ds)\n\u001b[1;32m    222\u001b[0m )\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE,\n\u001b[1;32m    229\u001b[0m )\n\u001b[0;32m--> 231\u001b[0m dataset \u001b[38;5;241m=\u001b[39m sequences_from_indices(data, indices, start_index, end_index)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     indices \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip(\n\u001b[1;32m    234\u001b[0m         (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;28mlen\u001b[39m(start_positions)), positions_ds)\n\u001b[1;32m    235\u001b[0m     )\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m i, positions: positions[i],\n\u001b[1;32m    237\u001b[0m         num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE,\n\u001b[1;32m    238\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.12/site-packages/keras/src/utils/timeseries_dataset_utils.py:256\u001b[0m, in \u001b[0;36msequences_from_indices\u001b[0;34m(array, indices_ds, start_index, end_index)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msequences_from_indices\u001b[39m(array, indices_ds, start_index, end_index):\n\u001b[0;32m--> 256\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensors(array[start_index:end_index])\n\u001b[1;32m    257\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip((dataset\u001b[38;5;241m.\u001b[39mrepeat(), indices_ds))\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m steps, inds: tf\u001b[38;5;241m.\u001b[39mgather(steps, inds),\n\u001b[1;32m    259\u001b[0m         num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE,\n\u001b[1;32m    260\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.12/site-packages/tensorflow/python/data/ops/dataset_ops.py:741\u001b[0m, in \u001b[0;36mDatasetV2.from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# from_tensors_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensors_op\n\u001b[0;32m--> 741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m from_tensors_op\u001b[38;5;241m.\u001b[39m_from_tensors(tensors, name)\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.12/site-packages/tensorflow/python/data/ops/from_tensors_op.py:23\u001b[0m, in \u001b[0;36m_from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensors\u001b[39m(tensors, name):  \u001b[38;5;66;03m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _TensorDataset(tensors, name)\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.12/site-packages/tensorflow/python/data/ops/from_tensors_op.py:31\u001b[0m, in \u001b[0;36m_TensorDataset.__init__\u001b[0;34m(self, element, name)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `tf.data.Dataset.from_tensors` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m   element \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mnormalize_element(element)\n\u001b[1;32m     32\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m     33\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure, element)\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.12/site-packages/tensorflow/python/data/util/structure.py:134\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    133\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 134\u001b[0m             ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponent_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m i, dtype\u001b[38;5;241m=\u001b[39mdtype))\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.12/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:713\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    712\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(\n\u001b[1;32m    714\u001b[0m     value, dtype, name, as_ref, preferred_dtype, accepted_result_types\n\u001b[1;32m    715\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m conversion_func(value, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname, as_ref\u001b[38;5;241m=\u001b[39mas_ref)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m constant_op\u001b[38;5;241m.\u001b[39mconstant(v, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    277\u001b[0m                         allow_broadcast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    293\u001b[0m )\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[1;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "# Step 1: Increase sequence length to use more time context\n",
    "sequence_length = 30  # instead of 1\n",
    "target = \"VICTIM_COUNT\"\n",
    "\n",
    "# Step 2: Prepare sliding window datasets correctly (no extra wrapper needed)\n",
    "train_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    data=train_df[features].values,\n",
    "    targets=train_df[target].values,\n",
    "    sequence_length=sequence_length,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "val_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    data=val_df[features].values,\n",
    "    targets=val_df[target].values,\n",
    "    sequence_length=sequence_length,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Prefetch to improve performance\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Step 3: Rebuild model with larger hidden size and attention heads\n",
    "model = build_tft_model(\n",
    "    input_shape=(sequence_length, len(features)),\n",
    "    hidden_size=128,  # increase model capacity\n",
    "    attention_heads=8,\n",
    "    dropout=0.1  # reduce regularization slightly\n",
    ")\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Step 4: Retrain with updated config\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=30,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"✅ Model retrained with updated hyperparameters!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275bd8a7-6fd0-4ef6-b11b-11583d5f8ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15032a92-3dac-4ff6-8485-420f4e613579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
